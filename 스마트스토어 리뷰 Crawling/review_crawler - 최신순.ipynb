{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로운 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url을 입력하세요\n",
      "https://smartstore.naver.com/drmeta/products/4863786589\n",
      "크롤링할 페이지 수를 입력하세요\n",
      "150\n",
      "저장파일명을 입력하세요\n",
      "닥터메타 판토네신\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 101.0.4951\n",
      "Get LATEST chromedriver version for 101.0.4951 google-chrome\n",
      "Driver [C:\\Users\\onrik\\.wdm\\drivers\\chromedriver\\win32\\101.0.4951.41\\chromedriver.exe] found in cache\n",
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_19760/1774285985.py:23: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n",
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_19760/1774285985.py:39: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//*[@id=\"_productTabContainer\"]/div/div[3]/ul/li[2]/a').click()\n",
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_19760/1774285985.py:41: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//*[@id=\"REVIEW\"]/div/div[3]/div/div[1]/div[1]/ul/li[2]/a').click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 페이지 크롤링 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_19760/1774285985.py:80: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//*[@id=\"REVIEW\"]/div/div[3]/div/div[2]/div/div/a[{}]'.format(counter)).click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 페이지 크롤링 시작\n",
      "3 페이지 크롤링 시작\n",
      "4 페이지 크롤링 시작\n",
      "5 페이지 크롤링 시작\n",
      "6 페이지 크롤링 시작\n",
      "7 페이지 크롤링 시작\n",
      "8 페이지 크롤링 시작\n",
      "9 페이지 크롤링 시작\n",
      "10 페이지 크롤링 시작\n",
      "11 페이지 크롤링 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onrik\\AppData\\Local\\Temp/ipykernel_19760/1774285985.py:89: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  driver.find_element_by_xpath('//*[@id=\"REVIEW\"]/div/div[3]/div/div[2]/div/div/a[{}]'.format(counter)).click()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 페이지 크롤링 시작\n",
      "13 페이지 크롤링 시작\n",
      "14 페이지 크롤링 시작\n",
      "15 페이지 크롤링 시작\n",
      "16 페이지 크롤링 시작\n",
      "17 페이지 크롤링 시작\n",
      "18 페이지 크롤링 시작\n",
      "19 페이지 크롤링 시작\n",
      "20 페이지 크롤링 시작\n",
      "21 페이지 크롤링 시작\n",
      "22 페이지 크롤링 시작\n",
      "23 페이지 크롤링 시작\n",
      "24 페이지 크롤링 시작\n",
      "25 페이지 크롤링 시작\n",
      "26 페이지 크롤링 시작\n",
      "27 페이지 크롤링 시작\n",
      "28 페이지 크롤링 시작\n",
      "29 페이지 크롤링 시작\n",
      "30 페이지 크롤링 시작\n",
      "31 페이지 크롤링 시작\n",
      "32 페이지 크롤링 시작\n",
      "33 페이지 크롤링 시작\n",
      "34 페이지 크롤링 시작\n",
      "35 페이지 크롤링 시작\n",
      "36 페이지 크롤링 시작\n",
      "37 페이지 크롤링 시작\n",
      "38 페이지 크롤링 시작\n",
      "39 페이지 크롤링 시작\n",
      "40 페이지 크롤링 시작\n",
      "41 페이지 크롤링 시작\n",
      "42 페이지 크롤링 시작\n",
      "43 페이지 크롤링 시작\n",
      "44 페이지 크롤링 시작\n",
      "45 페이지 크롤링 시작\n",
      "46 페이지 크롤링 시작\n",
      "47 페이지 크롤링 시작\n",
      "48 페이지 크롤링 시작\n",
      "49 페이지 크롤링 시작\n",
      "50 페이지 크롤링 시작\n",
      "51 페이지 크롤링 시작\n",
      "52 페이지 크롤링 시작\n",
      "53 페이지 크롤링 시작\n",
      "54 페이지 크롤링 시작\n",
      "55 페이지 크롤링 시작\n",
      "56 페이지 크롤링 시작\n",
      "57 페이지 크롤링 시작\n",
      "58 페이지 크롤링 시작\n",
      "59 페이지 크롤링 시작\n",
      "60 페이지 크롤링 시작\n",
      "61 페이지 크롤링 시작\n",
      "62 페이지 크롤링 시작\n",
      "63 페이지 크롤링 시작\n",
      "64 페이지 크롤링 시작\n",
      "65 페이지 크롤링 시작\n",
      "66 페이지 크롤링 시작\n",
      "67 페이지 크롤링 시작\n",
      "68 페이지 크롤링 시작\n",
      "69 페이지 크롤링 시작\n",
      "70 페이지 크롤링 시작\n",
      "71 페이지 크롤링 시작\n",
      "72 페이지 크롤링 시작\n",
      "73 페이지 크롤링 시작\n",
      "74 페이지 크롤링 시작\n",
      "75 페이지 크롤링 시작\n",
      "76 페이지 크롤링 시작\n",
      "77 페이지 크롤링 시작\n",
      "78 페이지 크롤링 시작\n",
      "79 페이지 크롤링 시작\n",
      "80 페이지 크롤링 시작\n",
      "81 페이지 크롤링 시작\n",
      "82 페이지 크롤링 시작\n",
      "83 페이지 크롤링 시작\n",
      "84 페이지 크롤링 시작\n",
      "85 페이지 크롤링 시작\n",
      "86 페이지 크롤링 시작\n",
      "87 페이지 크롤링 시작\n",
      "88 페이지 크롤링 시작\n",
      "89 페이지 크롤링 시작\n",
      "90 페이지 크롤링 시작\n",
      "91 페이지 크롤링 시작\n",
      "92 페이지 크롤링 시작\n",
      "93 페이지 크롤링 시작\n",
      "94 페이지 크롤링 시작\n",
      "95 페이지 크롤링 시작\n",
      "96 페이지 크롤링 시작\n",
      "97 페이지 크롤링 시작\n",
      "98 페이지 크롤링 시작\n",
      "99 페이지 크롤링 시작\n",
      "100 페이지 크롤링 시작\n",
      "101 페이지 크롤링 시작\n",
      "102 페이지 크롤링 시작\n",
      "103 페이지 크롤링 시작\n",
      "104 페이지 크롤링 시작\n",
      "105 페이지 크롤링 시작\n",
      "106 페이지 크롤링 시작\n",
      "107 페이지 크롤링 시작\n",
      "108 페이지 크롤링 시작\n",
      "109 페이지 크롤링 시작\n",
      "110 페이지 크롤링 시작\n",
      "111 페이지 크롤링 시작\n",
      "112 페이지 크롤링 시작\n",
      "113 페이지 크롤링 시작\n",
      "114 페이지 크롤링 시작\n",
      "115 페이지 크롤링 시작\n",
      "116 페이지 크롤링 시작\n",
      "117 페이지 크롤링 시작\n",
      "118 페이지 크롤링 시작\n",
      "119 페이지 크롤링 시작\n",
      "120 페이지 크롤링 시작\n",
      "121 페이지 크롤링 시작\n",
      "122 페이지 크롤링 시작\n",
      "123 페이지 크롤링 시작\n",
      "124 페이지 크롤링 시작\n",
      "125 페이지 크롤링 시작\n",
      "126 페이지 크롤링 시작\n",
      "127 페이지 크롤링 시작\n",
      "128 페이지 크롤링 시작\n",
      "129 페이지 크롤링 시작\n",
      "130 페이지 크롤링 시작\n",
      "131 페이지 크롤링 시작\n",
      "132 페이지 크롤링 시작\n",
      "133 페이지 크롤링 시작\n",
      "134 페이지 크롤링 시작\n",
      "135 페이지 크롤링 시작\n",
      "136 페이지 크롤링 시작\n",
      "137 페이지 크롤링 시작\n",
      "138 페이지 크롤링 시작\n",
      "139 페이지 크롤링 시작\n",
      "140 페이지 크롤링 시작\n",
      "141 페이지 크롤링 시작\n",
      "142 페이지 크롤링 시작\n",
      "143 페이지 크롤링 시작\n",
      "144 페이지 크롤링 시작\n",
      "145 페이지 크롤링 시작\n",
      "146 페이지 크롤링 시작\n",
      "147 페이지 크롤링 시작\n",
      "148 페이지 크롤링 시작\n",
      "149 페이지 크롤링 시작\n",
      "150 페이지 크롤링 시작\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "print('url을 입력하세요') #크롤링 할 링크 입력\n",
    "site_url = input()\n",
    "print('크롤링할 페이지 수를 입력하세요') #(네이버 리뷰 수 / 20) -> 올림\n",
    "page = int(input())\n",
    "print('저장파일명을 입력하세요')\n",
    "file_name=input()\n",
    "review_df=pd.DataFrame(columns=['id','passwd','리뷰','별점'])\n",
    "\n",
    "# driver=webdriver.Firefox(executable_path='/Users/parkjaeho/Desktop/온누리아이코리아/테크팀/5. Dump/geckodriver')\n",
    "\n",
    "# driver = Chrome('C:\\\\Users\\\\onrik\\\\chromedriver.exe',chrome_options=options) #sometimes you have to insert your execution path\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(site_url)\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # Wait to load page\n",
    "    time.sleep(3.5)\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "time.sleep(3)\n",
    "driver.find_element_by_xpath('//*[@id=\"_productTabContainer\"]/div/div[3]/ul/li[2]/a').click()\n",
    "time.sleep(3)\n",
    "driver.find_element_by_xpath('//*[@id=\"REVIEW\"]/div/div[3]/div/div[1]/div[1]/ul/li[2]/a').click()\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "reviewer=[]\n",
    "star_point=[]\n",
    "review_content=[]\n",
    "date = []\n",
    "counter =3\n",
    "page_num = 1\n",
    "for i in range(page):\n",
    "    print(page_num,'페이지 크롤링 시작')\n",
    "    time.sleep(1)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    review = soup.find('ul', class_='TsOLil1PRz')\n",
    "    r = review.select('li > div > div > div > div > div > div > div > div > span')\n",
    "    n = review.select('li > div > div > div > div > div > div > div > div > div > strong')\n",
    "    d = review.select('li > div > div > div > div > div > div > div > div > div > span')\n",
    "    s = review.select('li > div > div > div > div > div > div > div > div > div > em')\n",
    "    for ns in n:\n",
    "        name = ns.text\n",
    "        reviewer.append(name)\n",
    "    for rs in r:\n",
    "        review = rs.text\n",
    "        if review == 'BEST' or review =='재구매' or review == '한달사용기':\n",
    "            pass\n",
    "        else:\n",
    "            review_content.append(review)\n",
    "    for ss in s:\n",
    "        star=ss.text\n",
    "        star_point.append(star)\n",
    "    for dd in d:\n",
    "        if dd.text == '평점':\n",
    "            pass\n",
    "        else:\n",
    "            dates = dd.text\n",
    "            date.append(dates)\n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"REVIEW\"]/div/div[3]/div/div[2]/div/div/a[{}]'.format(counter)).click()       \n",
    "        counter += 1\n",
    "        page_num += 1\n",
    "        time.sleep(2)\n",
    "    except (NoSuchElementException, ElementNotInteractableException):\n",
    "        if page_num >= page:\n",
    "            print('끝!')\n",
    "        elif (counter)%13 == 0:\n",
    "            counter = 3\n",
    "            driver.find_element_by_xpath('//*[@id=\"REVIEW\"]/div/div[3]/div/div[2]/div/div/a[{}]'.format(counter)).click()       \n",
    "            counter += 1\n",
    "            page_num += 1\n",
    "    time.sleep(2)\n",
    "final=zip(reviewer, review_content, star_point, date)\n",
    "for i in final:\n",
    "    idc = i[0][:10]\n",
    "    reviewc=i[1]\n",
    "    starc=int(i[2])\n",
    "    dates = str(i[3])\n",
    "    review_df = review_df.append(pd.DataFrame([[idc,1111,reviewc,starc, dates]], columns=['id','passwd','리뷰','별점', '날짜']))\n",
    "review_df\n",
    "review_df.to_excel('./{}.xlsx'.format(file_name),index=False)\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000/20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
